---
title: "Мат. моделирование. Упражнение №8"
author: "Розумнюк А.А."
date: '23 апреля 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
library('tree')  
library('ISLR') 
library('randomForest') 
library('gbm')
```

# Деревья решений

Загрузим таблицу с данными по зарплатам и добавим к ней переменную High – “высокая зарплата” со значениями:

1 если зарплата больше 128.68;
0 в противном случае;

```{r, echo=TRUE}
attach(Wage)
# новая переменная
High <- ifelse(wage >= 128.68, '1', '0')
# присоединяем к таблице данных
Wage <- data.frame(Wage, High)
# модель бинарного  дерева
tree.wage <- tree(High ~ . -wage -region -logwage, Wage)
summary(tree.wage)

# график результата
plot(tree.wage)            # ветви
text(tree.wage, pretty=0)  # подписи

tree.wage                # посмотреть всё дерево в консоли

# ядро генератора случайных чисел
set.seed(6)
# обучающая выборка
train <- sample(1:nrow(Wage), 1500)
# тестовая выборка
Wage.test <- Wage[-train,]
High.test <- High[-train]

# строим дерево на обучающей выборке
tree.wage <- tree(High ~ . -wage -region -logwage, Wage, subset = train)

# делаем прогноз
tree.pred <- predict(tree.wage, Wage.test, type = "class")

# матрица неточностей
tbl <- table(tree.pred, High.test)
tbl

# оценка точности
acc.test <- sum(diag(tbl))/sum(tbl)
acc.test
```

Доля верных прогнозов: 0.774.

Теперь обрезаем дерево, используя в качестве критерия частоту ошибок классификации. Функция cv.tree() проводит кросс-валидацию для выбора лучшего дерева, аргумент prune.misclass означает, что мы минимизируем ошибку классификации.

```{r, echo=TRUE}
cv.wage <- cv.tree(tree.wage, FUN = prune.misclass)
# имена элементов полученного объекта
names(cv.wage)

cv.wage

# графики изменения параметров метода по ходу обрезки дерева ###################

# 1. ошибка с кросс-валидацией в зависимости от числа узлов
par(mfrow = c(1, 2))
plot(cv.wage$size, cv.wage$dev, type = "b",
     ylab = 'Частота ошибок с кросс-вал. (dev)',
     xlab = 'Число узлов (size)')
# размер дерева с минимальной ошибкой
opt.size <- cv.wage$size[cv.wage$dev == min(cv.wage$dev)]
abline(v = opt.size, col = 'red', 'lwd' = 2)     # соотв. вертикальная прямая
mtext(opt.size, at = opt.size, side = 1, col = 'red', line = 1)

# 2. ошибка с кросс-валидацией в зависимости от штрафа на сложность
plot(cv.wage$k, cv.wage$dev, type = "b",
     ylab = 'Частота ошибок с кросс-вал. (dev)',
     xlab = 'Штраф за сложность (k)')
```

Как видно на графике слева, минимум частоты ошибок достигается при числе узлов 3, 4 и 6. Выбираем минимальное значение.

```{r, echo=TRUE}
# дерево с 3 узлами
prune.wage <- prune.misclass(tree.wage, best = 3)

# визуализация
plot(prune.wage)
text(prune.wage, pretty = 0)

# прогноз на тестовую выборку
tree.pred <- predict(prune.wage, Wage.test, type = "class")

# матрица неточностей
tbl <- table(tree.pred, High.test)
tbl

# оценка точности
acc.test <- sum(diag(tbl))/sum(tbl)
acc.test

par(mfrow = c(1, 1))
```

# Дерево с обрезкой ветвей

Построим дерево регрессии для зависимой переменной wage. Не учитываем переменную High, с которой мы ранее имели дело.

```{r, echo=TRUE}
# обучающая выборка
train <- sample(1:nrow(Wage), nrow(Wage)/2) # обучающая выборка -- 50%

# обучаем модель
tree.wage <- tree(wage ~ . -High -region -logwage, Wage, subset = train)
summary(tree.wage)

# визуализация
plot(tree.wage)
text(tree.wage, pretty = 0)
```

Снова сделаем обрезку дерева в целях улучшения качества прогноза.

```{r, echo=TRUE}
cv.wage <- cv.tree(tree.wage)

# размер дерева с минимальной ошибкой
plot(cv.wage$size, cv.wage$dev, type = 'b', 
     ylab = 'Частота ошибок с кросс-вал. (dev)', 
     xlab = 'Число узлов (size)')
opt.size <- cv.wage$size[cv.wage$dev == min(cv.wage$dev)]
abline(v = opt.size, col = 'red', 'lwd' = 2)     # соотв. вертикальная прямая
mtext(opt.size, at = opt.size, side = 1, col = 'red', line = 1)
```

В данном случаем минимум ошибки соответствует самому сложному дереву, с 11 узлами.

```{r, echo=TRUE}
# дерево с 11 узлами
prune.wage = prune.tree(tree.wage, best = 11)
# визуализация
plot(prune.wage)
text(prune.wage, pretty = 0)
```

Делаем прогноз на тестовую выборку.

```{r, echo=TRUE}
# прогноз по лучшей модели
yhat <- predict(tree.wage, newdata = Wage[-train, ])
wage.test <- Wage[-train, "wage"]
# график "прогноз -- реализация"
plot(yhat, wage.test)
# линия идеального прогноза
abline(0, 1)
# MSE на тестовой выборке
mse.test <- mean((yhat - wage.test)^2)
mse.test
```

MSE на тестовой выборке равна 1273.502.